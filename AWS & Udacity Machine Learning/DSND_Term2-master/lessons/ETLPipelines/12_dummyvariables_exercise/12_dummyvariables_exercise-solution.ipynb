{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Variables Exercise\n",
    "\n",
    "In this exercise, you'll create dummy variables from the projects data set. The idea is to transform categorical data like this:\n",
    "\n",
    "| Project ID | Project Category |\n",
    "|------------|------------------|\n",
    "| 0          | Energy           |\n",
    "| 1          | Transportation   |\n",
    "| 2          | Health           |\n",
    "| 3          | Employment       |\n",
    "\n",
    "into new features that look like this:\n",
    "\n",
    "| Project ID | Energy | Transportation | Health | Employment |\n",
    "|------------|--------|----------------|--------|------------|\n",
    "| 0          | 1      | 0              | 0      | 0          |\n",
    "| 1          | 0      | 1              | 0      | 0          |\n",
    "| 2          | 0      | 0              | 1      | 0          |\n",
    "| 3          | 0      | 0              | 0      | 1          |\n",
    "\n",
    "\n",
    "(Note if you were going to use this data with a model influenced by multicollinearity, you would want to eliminate one of the columns to avoid redundant information.) \n",
    "\n",
    "The reasoning behind these transformations is that machine learning algorithms read in numbers not text. Text needs to be converted into numbers. You could assign a number to each category like 1, 2, 3, and 4. But categorical variable has no inherent order.\n",
    "\n",
    "Pandas makes it very easy to create dummy variables with the [get_dummies](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) method. In this exercise, you'll create dummy variables from the World Bank projects data; however, there's a caveat. The World Bank data is not particularly clean, so you'll need to explore and wrangle the data first.\n",
    "\n",
    "You'll focus on the text values in the sector variables.\n",
    "\n",
    "Run the code cells below to read in the World Bank projects data set and then to filter out the data for text variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# read in the projects data set and do basic wrangling \n",
    "projects = pd.read_csv('../data/projects_data.csv', dtype=str)\n",
    "projects.drop('Unnamed: 56', axis=1, inplace=True)\n",
    "projects['totalamt'] = pd.to_numeric(projects['totalamt'].str.replace(',', ''))\n",
    "projects['countryname'] = projects['countryname'].str.split(';', expand=True)[0]\n",
    "projects['boardapprovaldate'] = pd.to_datetime(projects['boardapprovaldate'])\n",
    "\n",
    "# keep the project name, lending, sector and theme data\n",
    "sector = projects.copy()\n",
    "sector = sector[['project_name', 'lendinginstr', 'sector1', 'sector2', 'sector3', 'sector4', 'sector5', 'sector',\n",
    "          'mjsector1', 'mjsector2', 'mjsector3', 'mjsector4', 'mjsector5',\n",
    "          'mjsector', 'theme1', 'theme2', 'theme3', 'theme4', 'theme5', 'theme ',\n",
    "          'goal', 'financier', 'mjtheme1name', 'mjtheme2name', 'mjtheme3name',\n",
    "          'mjtheme4name', 'mjtheme5name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below. This cell shows the percentage of each variable that is null. Notice the mjsector1 through mjsector5 variables are all null. The mjtheme1name through mjtheme5name are also all null as well as the theme variable. \n",
    "\n",
    "Because these variables contain so many null values, they're probably not very useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output percentage of values that are missing\n",
    "100 * sector.isnull().sum() / sector.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sector1 variable looks promising; it doesn't contain any null values at all. In the next cell, store the unique sector1 values in a list and output the results. Use the sort_values() and unique() methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a list of the unique values in sector1. Use the sort_values() and unique() pandas methods. \n",
    "# And then convert those results into a Python list\n",
    "uniquesectors1 = list(sector['sector1'].sort_values().unique())\n",
    "uniquesectors1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell to see the number of unique values\n",
    "print('Number of unique values in sector1:', len(uniquesectors1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3060 different categories is quite a lot! Remember that with dummy variables, if you have n categorical values, you need n - 1 new variables! That means 3060 extra columns! \n",
    "\n",
    "There are a few issues with this 'sector1' variable. First, there are values labeled '!$!0'. These should be substituted with NaN.\n",
    "\n",
    "Furthermore, each sector1 value ends with a ten or eleven character string like '!$!49!$!EP'. Some sectors show up twice in the list like:\n",
    " 'Other Industry; Trade and Services!$!70!$!YZ',\n",
    " 'Other Industry; Trade and Services!$!63!$!YZ',\n",
    "\n",
    "But it seems like those are actually the same sector. You'll need to remove everything past the exclamation point. \n",
    "\n",
    "Many values in the sector1 variable start with the term '(Historic)'. Try removing that phrase as well.\n",
    "\n",
    "Fix these issues in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In the sector1 variable, replace the string '!$!d0' with nan\n",
    "# HINT: you can use the pandas replace() method and numpy.nan\n",
    "sector['sector1'] = sector['sector1'].replace('!$!0', np.nan)\n",
    "\n",
    "# TODO: In the sector1 variable, remove the last 10 or 11 characters from the sector1 variable.\n",
    "# HINT: There is more than one way to do this including the replace method\n",
    "# HINT: You can use a regex expression '!.+'\n",
    "# That regex expression looks for a string with an exclamation\n",
    "# point followed by one or more characters\n",
    "\n",
    "sector['sector1'] = sector['sector1'].replace('!.+', '', regex=True)\n",
    "\n",
    "# TODO: Remove the string '(Historic)' from the sector1 variable\n",
    "# HINT: You can use the replace method\n",
    "sector['sector1'] = sector['sector1'].replace('^(\\(Historic\\))', '', regex=True)\n",
    "\n",
    "print('Number of unique sectors after cleaning:', len(list(sector['sector1'].unique())))\n",
    "print('Percentage of null values after cleaning:', 100 * sector['sector1'].isnull().sum() / sector['sector1'].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there are 156 unique categorical values. That's better than 3060. If you were going to use this data with a supervised learning machine model, you could try converting these 156 values to dummy variables. You'd still have to train and test a model to see if those are good features.\n",
    "\n",
    "But can you do anything else with the sector1 variable?\n",
    "\n",
    "The percentage of null values for 'sector1' is now 3.49%. That turns out to be the same number as the null values for the 'sector' column. You can see this if you scroll back up to where the code calculated the percentage of null values for each variable. \n",
    "\n",
    "Perhaps the 'sector1' and 'sector' variable have the same information. If you look at the 'sector' variable, however, it also needs cleaning. The values look like this:\n",
    "\n",
    "'Urban Transport;Urban Transport;Public Administration - Transportation'\n",
    "\n",
    "It turns out the 'sector' variable combines information from the 'sector1' through 'sector5' variables and the 'mjsector' variable. Run the code cell below to look at the sector variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector['sector']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What else can you do? If you look at all of the diferent sector1 categories, it might be useful to combine a few of them together. For example, there are various categories with the term \"Energy\" in them. And then there are other categories that seem related to energy but don't have the word energy in them like \"Thermal\" and \"Hydro\". Some categories have the term \"Renewable Energy\", so perhaps you could make a separate \"Renewable Energy\" category.\n",
    "\n",
    "Similarly, there are categories with the term \"Transportation\" in them, and then there are related categories like \"Highways\".\n",
    "\n",
    "In the next cell, find all sector1 values with the term 'Energy' in them. For each of these rows, put the string 'energy' in a new column called 'sector1_aggregates'. Do the same for \"Transportation\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Create the sector1_aggregates variable\n",
    "sector.loc[:,'sector1_aggregates'] = sector['sector1']\n",
    "\n",
    "# TODO: The code above created a new variable called sector1_aggregates. \n",
    "#       Currently, sector1_aggregates has all of the same values as sector1\n",
    "#       For this task, find all the rows in sector1_aggregates with the term 'Energy' in them, \n",
    "#       For all of these rows, replace whatever is the value is with the term 'Energy'.\n",
    "#       The idea is to simplify the category names by combining various categories together.\n",
    "#       Then, do the same for the term 'Transportation\n",
    "# HINT: You can use the contains() methods. See the documentation for how to ignore case using the re library\n",
    "# HINT: You might get an error saying \"cannot index with vector containing NA / NaN values.\" \n",
    "#       Try converting NaN values to something else like False or a string\n",
    "\n",
    "sector.loc[sector['sector1_aggregates'].str.contains('Energy', re.IGNORECASE).replace(np.nan, False),'sector1_aggregates'] = 'Energy'\n",
    "sector.loc[sector['sector1_aggregates'].str.contains('Transportation', re.IGNORECASE).replace(np.nan, False),'sector1_aggregates'] = 'Transportation'\n",
    "\n",
    "print('Number of unique sectors after cleaning:', len(list(sector['sector1_aggregates'].unique())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of unique sectors continues to go down. Keep in mind that how much to consolidate will depend on your machine learning model performance and your hardware's ability to handle the extra features in memory. If your hardware's memory can handle 3060 new features and your machine learning algorithm performs better, then go for it!\n",
    "\n",
    "There are still 638 entries with NaN values. How could you fill these in? You might try to determine an appropriate category from the 'project_name' or 'lendinginstr' variables. If you make dummy variables including NaN values, then you could consider a feature with all zeros to represent NaN. Or you could delete these records from the data set. Pandas will ignore NaN values by default. That means, for a given row, all dummy variables will have a value of 0 if the sector1 value was NaN.\n",
    "\n",
    "Don't forget about the bigger context! This data is being prepared for a machine learning algorithm. Whatever techniques you use to engineer new features, you'll need to use those when running your model on new data. So if your new data does not contain a sector1 value, you'll have to run whatever feature engineering processes you did on your training set.\n",
    "\n",
    "In this final set, use the pandas pd.get_dummies() method to create dummy variables. Then use the concat() method to concatenate the dummy variables to a dataframe that contains the project totalamt variable and the project year from the boardapprovaldate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create dummy variables from the sector1_aggregates data. Put the results into a dataframe called dummies\n",
    "# Hint: Use the get_dummies method\n",
    "dummies = pd.DataFrame(pd.get_dummies(sector['sector1_aggregates']))\n",
    "\n",
    "# TODO: Filter the projects data for the totalamt, the year from boardapprovaldate, and the dummy variables\n",
    "projects['year'] = projects['boardapprovaldate'].dt.year\n",
    "df = projects[['totalamt','year']]\n",
    "df_final = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Pandas makes it relatively easy to create dummy variables; however, oftentimes you'll need to clean the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
